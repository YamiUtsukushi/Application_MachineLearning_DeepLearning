# ğŸ“š Projet M2 ESI â€“ Machine Learning & Deep Learning

Ce projet est divisÃ© en **3 parties**, chacune explorant une facette du Machine Learning ou du Deep Learning Ã  travers un cas concret :

1. **Reconnaissance dâ€™images avec CNN (TensorFlow + Keras)**
2. **DÃ©tection de fraude avec donnÃ©es tabulaires (Scikit-learn)**
3. **Analyse de texte (NLP) avec LSTM (TensorFlow)**

---

## Partie 1 : Reconnaissance dâ€™Images avec les CNN

### ğŸ” Objectif
Construire un modÃ¨le CNN pour classifier les images du dataset CIFAR-10.

### ğŸ“¸ Captures d'Ã©cran

1. **Affichage dâ€™images du dataset**  
![Capture 1](./images/capture1.png)

2. **Console d'entraÃ®nement du modÃ¨le**  
![Capture 2](./images/capture2.png)

3. **Courbes dâ€™apprentissage (accuracy / loss)**  
![Capture 3](./images/capture3.png)

4. **Matrice de confusion**  
![Capture 4](./images/capture4.png)

5. **TensorBoard** *(optionnel)*  
![Capture 5](./images/capture5.png)

6. **Tableau de test dâ€™hyperparamÃ¨tres** *(si implÃ©mentÃ©)*  
![Capture 6](./images/capture6.png)

---

### â“ Questions / RÃ©ponses Partie 1

#### 1. Quel a Ã©tÃ© le rÃ´le des callbacks ModelCheckpoint et EarlyStopping ? Ont-ils Ã©tÃ© utiles pendant votre entraÃ®nement ?
> Un callback est une fonction qui va Ãªtre appelÃ© automatiquement Ã  chaque Ã©poque sans quâ€™on est Ã  intervenir manuellement.

> Le callback ModelCheckpoint sert Ã  rÃ©cupÃ©rer le meilleur modÃ¨le Ã  chaque passage. Lâ€™EarlyStopping lui, arrÃªte lâ€™apprentissage du modÃ¨le si celui-ci arrÃªte de sâ€™amÃ©liorer pendant un nombre dâ€™Ã©poque donnÃ©e.

> Oui les deux callbacks ont Ã©tÃ© utiles pendant lâ€™entrainement de notre modÃ¨le, ModelCheckpoint a rÃ©cupÃ©rÃ© la meilleure version de notre modÃ¨le et EarlyStopping a stoppÃ© lâ€™apprentissage de notre modÃ¨le aprÃ¨s 12 Ã©poques.


#### 2. Analysez les courbes d'apprentissage (perte et prÃ©cision). Votre modÃ¨le prÃ©sente-t-il des signes de sur-apprentissage ou de sous-apprentissage ? Comment la matrice de confusion vous aide-t-elle Ã  comprendre les erreurs du modÃ¨le ?
> Oui on aperÃ§oit des signes de sur-apprentissage, le modÃ¨le est trÃ¨s bon sur les donnÃ©es dâ€™entraÃ®nement, la prÃ©cision augmente au fil des Ã©poques. Cependant pour les donnÃ©es de validation, on voit quâ€™Ã  partir de lâ€™Ã©poque 3, notre modÃ¨le commence Ã  stagner.

> Sur le graph de perte, on voit que le modÃ¨le rÃ©duit bien lâ€™erreur des donnÃ©es dâ€™entraÃ®nement mais il y a de plus en plus sur les donnÃ©es de validations.

> On comprend que le modÃ¨le apprend et mÃ©morise trÃ¨s bien sur les donnÃ©es dâ€™entrainement mais a du mal sur les donnÃ©es inconnues ce qui serait un signe de sur-apprentissage.

> La matrice de confusion va nous permettre de savoir oÃ¹ le modÃ¨le se trompe entre les prÃ©dictions correctes et les erreurs de classification. On peut voir que les bonnes prÃ©dictions de notre modÃ¨le sur la ligne en diagonale. 

> Le modÃ¨le a beaucoup confondu chien/chat, cheval/cerf, camion/voiture il a du mal Ã  diffÃ©rencier les Ã©lÃ©ments qui se ressemble.


#### 3. Quels ont Ã©tÃ© les principaux dÃ©fis rencontrÃ©s ? Comment pourriez-vous amÃ©liorer les performances de ce premier modÃ¨le ?
> La comprÃ©hension des paramÃ¨tres lors de la crÃ©ation du modÃ¨le Ã©tait le dÃ©fi principal

> Pour amÃ©liorer notre modÃ¨le, on pourrait lui donner plus de donnÃ©es sur les classes mal reconnues et plus de variations sur ses images. Ajouter plus de couches de neurones et amÃ©liorer les hyperparamÃ¨tres du modÃ¨le (nombre de filtre, learning_rate etc).


#### 4. Comparez les performances (prÃ©cision, temps d'entraÃ®nement) du modÃ¨le simple et du modÃ¨le basÃ© sur le transfert d'apprentissage. Pourquoi observe-t-on une telle diffÃ©rence ?
> Le modÃ¨le CNN a mis 1 minute et 49 secondes pour sâ€™entraÃ®ner, tandis que le modÃ¨le basÃ© sur le transfert dâ€™apprentissage (MobileNetV2) a pris 6 minutes et 35 secondes

> MalgrÃ© un temps dâ€™entraÃ®nement beaucoup plus long, le modÃ¨le par transfert a obtenu des performances infÃ©rieures au CNN sur ce jeu de donnÃ©es. Cela sâ€™explique par le fait que seule la "tÃªte" du modÃ¨le a Ã©tÃ© entraÃ®nÃ©e, et que la base MobileNetV2 est initialement conÃ§ue pour des images plus grandes que celles du dataset CIFAR-10.

> En revanche, mÃªme si le CNN est plus performant ici, il montre des signes de sur-apprentissage, alors que le modÃ¨le par transfert reste plus stable et gÃ©nÃ©ralise mieux.

---

## Partie 2 : DonnÃ©es Tabulaires â€“ DÃ©tection de Fraude

### ğŸ“¸ Captures d'Ã©cran

1. **Analyse exploratoire des donnÃ©es (EDA)**  
![Capture 7](./images/capture7.1.png)

![Capture 7](./images/capture7.2.png)

![Capture 7](./images/capture7.3.png)

2. **Console d'entraÃ®nement du modÃ¨le RandomForest**  
![Capture 8](./images/capture8.png)

3. **Matrice de confusion + classification report**  
![Capture 9](./images/capture9.png)

---

### â“ Questions / RÃ©ponses Partie 2

#### 1. Expliquez le rÃ´le du paramÃ¨tre `class_weight='balanced'`. Quelles autres techniques auriez-vous pu utiliser pour gÃ©rer le dÃ©sÃ©quilibre des classes ?
> `class_weight='balanced'` sert a corriger le dÃ©sÃ©quilibre entre les classes fraude et non-fraude.

> On peut rÃ©duire le nombre d'exemples non frauduleux ou utliser d'autres algorithmes pour gÃ©rer le dÃ©sÃ©quilibre des classes comme XGBoost avec '`scale_pos_weight'`.

#### 2. Quel type d'apprentissage et quel type de problÃ¨me est-ce ? Argumentez.
> Il s'agit dâ€™un apprentissage supervisÃ© : on donne au modÃ¨le des exemples avec leur rÃ©sultat (fraude ou non).

> Le problÃ¨me est une classification binaire : on a deux classes (fraude ou non).
> - 0 â†’ transaction normale
> - 1 â†’ transaction frauduleuse 
> Ducoup le modÃ¨le apprend Ã  prÃ©dire une classe, et non une valeur numÃ©rique comme dans un problÃ¨me de rÃ©gression

#### 3. Donnez un exemple de rÃ©gression liÃ© Ã  ce type de donnÃ©es, mettez un exemple via argumentation et image.

> Un exemple de rÃ©gression serait de prÃ©dire le montant probable dâ€™une prochaine transaction ou estimer le montant moyen attendu sur une pÃ©riode donnÃ©e, ou encore le nombre de transactions futures pour un client.

> Cela permettrait de repÃ©rer les montants anormalement Ã©levÃ©s ou faibles par rapport au profil habituel, et donc de renforcer la dÃ©tection de fraudes.

> Par exemple, si un utilisateur a gÃ©nÃ©ralement des transactions autour de 15â‚¬, mais que le modÃ¨le prÃ©dit 19â‚¬ et quâ€™il envoie une transaction de 3 000â‚¬, on peut dÃ©tecter ce probleme et la mettre en alerte.

![Capture 12](./images/p2_argumentation.png)
---

## ğŸ“ Arborescence du dÃ©pÃ´t

```
ML_DL/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ cam.png
â”‚   â”œâ”€â”€ capture1.png
â”‚   â”œâ”€â”€ capture2.png
â”‚   â”œâ”€â”€ capture3.png
â”‚   â”œâ”€â”€ capture4.png
â”‚   â”œâ”€â”€ capture5.png
â”‚   â”œâ”€â”€ capture7.1.png
â”‚   â”œâ”€â”€ capture7.2.png
â”‚   â”œâ”€â”€ capture7.3.png
â”‚   â”œâ”€â”€ capture8.png
â”‚   â”œâ”€â”€ capture9.png
â”‚   â””â”€â”€ p2_argumentation.png
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ (fichiers gÃ©nÃ©rÃ©s par TensorBoard)
â”œâ”€â”€ best_cnn_model.keras
â”œâ”€â”€ best_transfer_model.keras
â”œâ”€â”€ best_model_nlp.keras
â”œâ”€â”€ temp_model.keras
â”œâ”€â”€ creditcard.csv                         # Dataset pour la Partie 2
â”œâ”€â”€ personal_expense_classification.csv    # Dataset pour la Partie 3
â”œâ”€â”€ partie1_CNN.ipynb                      # Notebook Partie 1 (CIFAR-10)
â”œâ”€â”€ partie2_Tabulaires_RF.ipynb            # Notebook Partie 2 (Fraude)
â”œâ”€â”€ partie3_NLP_LSTM.ipynb                 # Notebook Partie 3 (NLP)
â”œâ”€â”€ README.md
```

---

## âœ‰ï¸ Auteurs
- Jayson, Sofiane, Hamidou, Antony et Ramdhan 
- Promo : Master 2 ESI
